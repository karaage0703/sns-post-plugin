"""ã¯ã¦ãªãƒ–ãƒ­ã‚°è¨˜äº‹åé›†ãƒ»é‡ã¿ä»˜ã‘ãƒ©ãƒ³ãƒ€ãƒ é¸å‡ºã‚·ã‚¹ãƒ†ãƒ """

import requests
import json
import random
import time
from datetime import datetime, timedelta
from bs4 import BeautifulSoup
from pathlib import Path
from typing import List, Dict, Any, Optional
import logging
from concurrent.futures import ThreadPoolExecutor, as_completed

logger = logging.getLogger(__name__)


class HatenaArchiveCrawler:
    def __init__(self, blog_url: str = "https://karaage.hatenadiary.jp", cache_file: Optional[str] = None):
        self.blog_url = blog_url

        if cache_file:
            self.cache_file = Path(cache_file)
        else:
            # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
            cache_dir = Path.home() / ".cache" / "sns-post-plugin"
            cache_dir.mkdir(parents=True, exist_ok=True)
            # ãƒ–ãƒ­ã‚°URLã‹ã‚‰ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ•ã‚¡ã‚¤ãƒ«åã‚’ç”Ÿæˆ
            blog_name = blog_url.replace("https://", "").replace("http://", "").replace("/", "_")
            self.cache_file = cache_dir / f"{blog_name}_cache.json"

        self.session = requests.Session()
        self.session.headers.update({"User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36"})

    def generate_archive_urls(self, start_year: int = 2014, end_year: Optional[int] = None) -> List[str]:
        """æœˆåˆ¥ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–URLã‚’ç”Ÿæˆ"""
        if end_year is None:
            end_year = datetime.now().year

        archive_urls = []
        for year in range(start_year, end_year + 1):
            for month in range(1, 13):
                if year == datetime.now().year and month > datetime.now().month:
                    break
                url = f"{self.blog_url}/archive/{year}/{month:02d}"
                archive_urls.append(url)
        return archive_urls

    def fetch_articles_from_archive(self, archive_url: str) -> List[Dict[str, Any]]:
        """æœˆåˆ¥ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–ãƒšãƒ¼ã‚¸ã‹ã‚‰è¨˜äº‹ãƒªãƒ³ã‚¯ã‚’æŠ½å‡º"""
        try:
            response = self.session.get(archive_url, timeout=10)
            response.raise_for_status()
            soup = BeautifulSoup(response.text, "html.parser")

            links = soup.select("a.entry-title-link")

            articles = []
            for link in links:
                title = link.text.strip()
                url = link.get("href")
                if url and title:
                    if url.startswith("/"):
                        url = self.blog_url + url

                    articles.append({"title": title, "url": url, "archive_url": archive_url})

            logger.info(f"âœ… {archive_url}: {len(articles)}ä»¶ã®è¨˜äº‹ã‚’å–å¾—")
            return articles

        except Exception as e:
            logger.error(f"âŒ ã‚¨ãƒ©ãƒ¼ {archive_url}: {e}")
            return []

    def collect_all_articles(self, start_year: int = 2014) -> List[Dict[str, Any]]:
        """å…¨æœŸé–“ã®è¨˜äº‹ã‚’åé›†"""
        archive_urls = self.generate_archive_urls(start_year)
        all_articles = []

        logger.info(f"ğŸ“š {len(archive_urls)}å€‹ã®ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–ãƒšãƒ¼ã‚¸ã‚’ã‚¯ãƒ­ãƒ¼ãƒ«é–‹å§‹...")

        for i, url in enumerate(archive_urls):
            articles = self.fetch_articles_from_archive(url)
            all_articles.extend(articles)

            if i % 10 == 0:
                time.sleep(1)

            if i % 50 == 0 and i > 0:
                logger.info(f"ğŸ“Š é€²æ—: {i}/{len(archive_urls)} ({len(all_articles)}ä»¶ã®è¨˜äº‹ã‚’åé›†æ¸ˆã¿)")

        logger.info(f"ğŸ‰ åé›†å®Œäº†: åˆè¨ˆ{len(all_articles)}ä»¶ã®è¨˜äº‹")
        return all_articles

    def get_hatena_bookmark_count(self, url: str) -> Optional[int]:
        """ã¯ã¦ãªãƒ–ãƒƒã‚¯ãƒãƒ¼ã‚¯æ•°ã‚’å–å¾—"""
        try:
            api_url = f"https://b.hatena.ne.jp/entry/jsonlite/?url={url}"
            response = self.session.get(api_url, timeout=10)

            if response.status_code == 200:
                response_text = response.text.strip()

                if not response_text or response_text == "null":
                    return 0

                try:
                    data = response.json()
                    if isinstance(data, dict):
                        return data.get("count", 0)
                except:
                    pass

            return None

        except Exception as e:
            if "timeout" not in str(e).lower():
                logger.error(f"âš ï¸ ãƒ–ãƒƒã‚¯ãƒãƒ¼ã‚¯æ•°å–å¾—ã‚¨ãƒ©ãƒ¼ {url}: {e}")
            return None

    def _fetch_single_bookmark_count(self, article: Dict[str, Any]) -> Dict[str, Any]:
        """å˜ä¸€è¨˜äº‹ã®ãƒ–ãƒƒã‚¯ãƒãƒ¼ã‚¯æ•°ã‚’å–å¾—ï¼ˆä¸¦åˆ—å‡¦ç†ç”¨ï¼‰"""
        if "bookmark_count" not in article:
            bookmark_count = self.get_hatena_bookmark_count(article["url"])
            article["bookmark_count"] = bookmark_count if bookmark_count is not None else 0
        return article

    def fetch_bookmark_counts(self, articles: List[Dict[str, Any]], max_workers: int = 20) -> List[Dict[str, Any]]:
        """è¨˜äº‹ã®ãƒ–ãƒƒã‚¯ãƒãƒ¼ã‚¯æ•°ã‚’ä¸¦åˆ—å–å¾—ï¼ˆé«˜é€ŸåŒ–ç‰ˆï¼‰"""
        logger.info(f"ğŸ”– {len(articles)}ä»¶ã®è¨˜äº‹ã®ãƒ–ãƒƒã‚¯ãƒãƒ¼ã‚¯æ•°ã‚’ä¸¦åˆ—å–å¾—ä¸­ï¼ˆ{max_workers}ã‚¹ãƒ¬ãƒƒãƒ‰ï¼‰...")
        success_count = 0
        error_count = 0
        bookmark_found_count = 0

        # ä¸¦åˆ—å‡¦ç†ã§ãƒ–ãƒƒã‚¯ãƒãƒ¼ã‚¯æ•°ã‚’å–å¾—
        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            future_to_article = {executor.submit(self._fetch_single_bookmark_count, article): article for article in articles}

            completed = 0
            for future in as_completed(future_to_article):
                try:
                    result = future.result()
                    bookmark_count = result.get("bookmark_count", 0)

                    if bookmark_count >= 0:
                        success_count += 1
                        if bookmark_count > 0:
                            bookmark_found_count += 1
                    else:
                        error_count += 1

                    completed += 1
                    if completed % 50 == 0:
                        logger.info(
                            f"ğŸ“Š é€²æ—: {completed}/{len(articles)} | æˆåŠŸ: {success_count}, ãƒ–ãƒƒã‚¯ãƒãƒ¼ã‚¯ã‚ã‚Š: {bookmark_found_count}"
                        )
                except Exception as e:
                    error_count += 1
                    logger.error(f"âŒ ãƒ–ãƒƒã‚¯ãƒãƒ¼ã‚¯å–å¾—ã‚¨ãƒ©ãƒ¼: {e}")

        success_rate = (success_count / len(articles)) * 100 if articles else 0
        logger.info(
            f"âœ… å®Œäº†: æˆåŠŸç‡ {success_rate:.1f}% ({success_count}/{len(articles)}) | ãƒ–ãƒƒã‚¯ãƒãƒ¼ã‚¯ã‚ã‚Š: {bookmark_found_count}ä»¶"
        )
        return articles

    def save_cache(self, articles: List[Dict[str, Any]]):
        """è¨˜äº‹ãƒ‡ãƒ¼ã‚¿ã‚’ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜"""
        cache_data = {"last_updated": datetime.now().isoformat(), "articles": articles}

        with open(self.cache_file, "w", encoding="utf-8") as f:
            json.dump(cache_data, f, ensure_ascii=False, indent=2)

        logger.info(f"ğŸ’¾ ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ä¿å­˜: {self.cache_file}")

    def load_cache(self) -> Optional[List[Dict[str, Any]]]:
        """ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰è¨˜äº‹ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿"""
        if not self.cache_file.exists():
            return None

        try:
            with open(self.cache_file, "r", encoding="utf-8") as f:
                cache_data = json.load(f)

            last_updated = datetime.fromisoformat(cache_data["last_updated"])
            if datetime.now() - last_updated > timedelta(days=1):
                logger.info("â° ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãŒå¤ã„ãŸã‚æ›´æ–°ã—ã¾ã™")
                return None

            logger.info(f"ğŸ“‹ ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‹ã‚‰{len(cache_data['articles'])}ä»¶ã®è¨˜äº‹ã‚’èª­ã¿è¾¼ã¿")
            return cache_data["articles"]

        except Exception as e:
            logger.error(f"âŒ ã‚­ãƒ£ãƒƒã‚·ãƒ¥èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
            return None

    def weighted_random_selection(self, articles: List[Dict[str, Any]], exclude_recent_days: int = 30) -> Dict[str, Any]:
        """é‡ã¿ä»˜ã‘ãƒ©ãƒ³ãƒ€ãƒ é¸å‡º"""
        cutoff_date = datetime.now() - timedelta(days=exclude_recent_days)

        bookmarked_articles = [article for article in articles if article.get("bookmark_count", 0) > 0]
        no_bookmark_articles = [article for article in articles if article.get("bookmark_count", 0) == 0]

        if bookmarked_articles:
            if random.random() < 0.7 and bookmarked_articles:
                selected_articles = bookmarked_articles
                weights = [article.get("bookmark_count", 0) + 1 for article in selected_articles]
                selection_type = "ãƒ–ãƒƒã‚¯ãƒãƒ¼ã‚¯é‡ã¿ä»˜ã"
            else:
                selected_articles = no_bookmark_articles if no_bookmark_articles else articles
                weights = [1 for _ in selected_articles]
                selection_type = "ãƒ©ãƒ³ãƒ€ãƒ "
        else:
            selected_articles = articles
            weights = [1 for _ in selected_articles]
            selection_type = "å…¨è¨˜äº‹ãƒ©ãƒ³ãƒ€ãƒ "

        if not selected_articles:
            selected_articles = articles
            weights = [1 for _ in selected_articles]
            selection_type = "ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯"

        selected_article = random.choices(selected_articles, weights=weights, k=1)[0]

        logger.info(f"ğŸ¯ é¸å‡ºã•ã‚ŒãŸè¨˜äº‹: {selected_article['title']}")
        logger.info(f"ğŸ“– ãƒ–ãƒƒã‚¯ãƒãƒ¼ã‚¯æ•°: {selected_article.get('bookmark_count', 0)}")
        logger.info(f"ğŸ² é¸æŠæ–¹æ³•: {selection_type}")

        return selected_article

    def run_full_crawl(self, start_year: int = 2014, use_cache: bool = True) -> tuple[Dict[str, Any], List[Dict[str, Any]]]:
        """ãƒ•ãƒ«ã‚¯ãƒ­ãƒ¼ãƒ«å®Ÿè¡Œ"""
        cached_articles = self.load_cache() if use_cache else None

        if cached_articles:
            articles = cached_articles
            logger.info(f"ğŸ“‹ ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‹ã‚‰{len(articles)}ä»¶ã®è¨˜äº‹ã‚’ä½¿ç”¨")
        else:
            articles = self.collect_all_articles(start_year=start_year)
            articles = self.fetch_bookmark_counts(articles)
            self.save_cache(articles)

        selected_article = self.weighted_random_selection(articles)

        return selected_article, articles
